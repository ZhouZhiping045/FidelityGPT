# FidelityGPT Artifact README

## üìÅ Artifact Structure

```
‚îú‚îÄ‚îÄ Dataset/                     # Decompiled functions (///// separated)
‚îú‚îÄ‚îÄ Ground truth/               # Ground truth functions (///// separated)
‚îú‚îÄ‚îÄ Evaluation/                 # Evaluation script
‚îú‚îÄ‚îÄ testdata/                   # Raw test data (txt)
‚îú‚îÄ‚îÄ config.ini                  # System configuration
‚îú‚îÄ‚îÄ FidelityGPT.py              # Distortion detection
‚îú‚îÄ‚îÄ Correction.py               # Distortion correction
‚îú‚îÄ‚îÄ prompt_templates.py         # Prompt templates for all LLM tasks
‚îú‚îÄ‚îÄ pattern_matcher.py          # Dynamic Semantic Intensity Retrieval Algorithm
‚îú‚îÄ‚îÄ variabledependency.py       # Variable Dependency Algorithm
‚îú‚îÄ‚îÄ document_processor.py       # Utility for document formatting
‚îú‚îÄ‚îÄ embedding_retriever.py      # RAG embedding retrieval logic
‚îú‚îÄ‚îÄ fidelity_new.c              # Distortion DB (for IDA Pro)
‚îú‚îÄ‚îÄ fidelity_ghidra.c           # Distortion DB (for Ghidra)
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ README.md                   # This file
```

## ‚öôÔ∏è Configuration

Before running the system, update `config.ini`:

```ini
[LLM]
model = gpt-4o
temperature = 0
api_key = sk-XXXX             
api_base = XXXX 

[PATHS]
input_dir = Dataset_4_AE              ; Folder for input decompiled functions
output_dir = Dataset_4_AE_output      ; Folder for storing detection/correction results
knowledge_base = fidelity_new.c       ; Use `fidelity_ghidra.c` if using Ghidra
```

- Input functions: `.txt` files, each with functions separated by `/////`
- Distortion DB: `fidelity_new.c` (IDA Pro) or `fidelity_ghidra.c` (Ghidra)

## üß™ Step-by-Step Execution

### 1. Prepare Input Files

Move test functions into the input folder:

```bash
mkdir Dataset_4_AE
cp testdata/*.txt Dataset_4_AE/
```

> You may also copy `Dataset/*.txt` into `Dataset_4_AE/` if needed:
```bash
cp Dataset/*.txt Dataset_4_AE/
```

### 2. Run Distortion Detection

```bash
python FidelityGPT.py
```

- Input: Files from `Dataset_4_AE/`
- Output: Stored in `Dataset_4_AE_output/`
- Each line is labeled with distortion type `I1`‚Äì`I6`
- Functions are separated using `/////`

> ‚ÑπÔ∏è For functions longer than 50 lines, the system uses **chunk-based detection** with a 5-line overlap.  
After detection (before running Correction or Evaluation):

After detection:
- Manually merge chunked functions
- Remove overlapping duplicate lines
- Preserve the ///// separator

‚ö†Ô∏è Ensure line alignment: each line in model_output.txt should correspond exactly to the same function segment in ground_truth.txt. Since functions longer than 50 lines were split into chunks during preprocessing, manual alignment may be required.

üëâ In our practice, we place ground_truth.txt and model_output.txt in two columns of an Excel sheet to ensure proper alignment before running Correction or Evaluation.

### 3. Run Correction

```bash
python Correction.py
```

### 4. Run Evaluation

Ensure the following:
- `Ground truth/ground_truth.txt`: aligned reference results
- `model_output.txt`: post-processed output from detection (line-aligned)

Then run:

```bash
python Evaluation/Evaluation.py

For the correction phase, manual evaluation is required. Please refer to Table I in the paper as the guideline for manual assessment.

## üß† Key Components

| Script | Description |
|--------|-------------|
| `FidelityGPT.py` | Main detection pipeline |
| `Correction.py` | Fixes distorted code lines |
| `prompt_templates.py` | LLM prompt templates |
| `pattern_matcher.py` | Semantic intensity retrieval |
| `variabledependency.py` | Variable dependency analysis |
| `Evaluation/Evaluation.py` | Compares against ground truth |
| `fidelity_new.c` / `fidelity_ghidra.c` | RAG knowledge base |

## ‚úÖ Requirements

```bash
pip install -r requirements.txt
```
